{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import os\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from file\n",
    "\n",
    "#Read training data\n",
    "DATA_PATH = os.path.join('..','data') # use '..' to go up one directory level\n",
    "SOURCE = ''\n",
    "NAME = 'augmented_beos.csv'\n",
    "training_data = pd.read_csv(os.path.join(DATA_PATH, SOURCE, NAME)).dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checking the data\n",
    "class_count = training_data['diseases'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode the label\n",
    "# label_encoder = LabelEncoder()\n",
    "# training_data['diseases'] = label_encoder.fit_transform(training_data['diseases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class = class_count.max()\n",
    "balance_size = max_class\n",
    "for disease in class_count.index:\n",
    "    if class_count[disease] < balance_size:\n",
    "        # new samples random drop a value 1 to 0\n",
    "        new_samples = training_data[training_data['diseases'] == disease][:].sample(n=balance_size - class_count[disease], replace=True)\n",
    "        # num_to_flip = random.randint(1,class_count[disease])\n",
    "        # ones_indices = np.argwhere(new_samples.values == 1)\n",
    "        # indices_to_flip = ones_indices[np.random.choice(ones_indices.shape[0], size=num_to_flip, replace=True)]\n",
    "        # for index in indices_to_flip:\n",
    "        #     new_samples.iat[index[0], index[1]] = 0\n",
    "        training_data = pd.concat([training_data[:], new_samples[:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"../data/augmented_biggest_real.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (753829, 377)\n",
      "X_test shape:  (188458, 377)\n",
      "y_train shape:  (753829,)\n",
      "y_test shape:  (188458,)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(training_data, test_size=0.2, random_state=42, stratify=training_data['diseases'])\n",
    "\n",
    "X_train = train_df.drop(columns=['diseases'])\n",
    "y_train = train_df['diseases']\n",
    "X_test = test_df.drop(columns=['diseases'])\n",
    "y_test = test_df['diseases']\n",
    "print(\"X_train shape: \", X_train.shape) \n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe = pd.get_dummies(y_train[:])[:]\n",
    "y_test_ohe = pd.get_dummies(y_test[:])[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_ohe_tensor = torch.tensor(y_train_ohe.values, dtype=torch.float32)\n",
    "y_test_ohe_tensor = torch.tensor(y_test_ohe.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensor = TensorDataset(X_train_tensor, y_train_ohe_tensor)\n",
    "testing_tensor = TensorDataset(X_test_tensor, y_test_ohe_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_loader = DataLoader(training_tensor, batch_size, shuffle=True)\n",
    "testing_loader = DataLoader(testing_tensor, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(X_train_tensor.shape[1], 2048)\n",
    "        self.fc2 = torch.nn.Linear(2048, 4096)\n",
    "        self.fc3 = torch.nn.Linear(4096, 2048)\n",
    "        self.fc4 = torch.nn.Linear(2048,1024)\n",
    "        self.fc5 = torch.nn.Linear(1024, 773)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    accuracy = 0\n",
    "\n",
    "    # Move model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over data\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Move data to GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "       \n",
    "        loss.backward()\n",
    "\n",
    "        # Compute the accuracy\n",
    "        # predicted = torch.argmax(outputs, 1)\n",
    "        # correct = (predicted == torch.argmax(labels, 1)).sum().item()\n",
    "        # accuracy += correct\n",
    "\n",
    "        #change to function\n",
    "        top5_indices = outputs.topk(5, dim=1).indices\n",
    "        for j in range(len(top5_indices)):\n",
    "            try:\n",
    "                accuracy += (top5_indices[j] == torch.argmax(labels[j])).sum().item()\n",
    "            except:\n",
    "                print(j)\n",
    "                print(top5_indices)\n",
    "                print(torch.argmax(labels[j]))\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            print(accuracy)\n",
    "\n",
    "    return last_loss, accuracy / len(training_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.27830568918405335\n",
      "31674\n",
      "  batch 2000 loss: 0.28556937208864835\n",
      "63312\n",
      "  batch 3000 loss: 0.24481931295990944\n",
      "95010\n",
      "  batch 4000 loss: 0.27645748547883703\n",
      "126707\n",
      "  batch 5000 loss: 0.2645503236782097\n",
      "158386\n",
      "  batch 6000 loss: 0.3222263813610189\n",
      "190062\n",
      "  batch 7000 loss: 0.2521832288055448\n",
      "221726\n",
      "  batch 8000 loss: 0.25412584209558553\n",
      "253393\n",
      "  batch 9000 loss: 0.25189784551458433\n",
      "285050\n",
      "  batch 10000 loss: 0.27936381357896606\n",
      "316685\n",
      "  batch 11000 loss: 0.26498294360283764\n",
      "348317\n",
      "  batch 12000 loss: 0.26045931453770027\n",
      "379958\n",
      "  batch 13000 loss: 0.26671574972057716\n",
      "411588\n",
      "  batch 14000 loss: 0.37020779536274495\n",
      "443216\n",
      "  batch 15000 loss: 0.2621216626699606\n",
      "474875\n",
      "  batch 16000 loss: 0.30343709855806084\n",
      "506517\n",
      "  batch 17000 loss: 0.25872717176307924\n",
      "538165\n",
      "  batch 18000 loss: 0.2568693648930639\n",
      "569839\n",
      "  batch 19000 loss: 0.272896213992266\n",
      "601517\n",
      "  batch 20000 loss: 0.2617223817824852\n",
      "633198\n",
      "  batch 21000 loss: 0.25837669902457855\n",
      "664882\n",
      "  batch 22000 loss: 0.2550807692222297\n",
      "696524\n",
      "  batch 23000 loss: 0.3745530421845615\n",
      "728173\n",
      "LOSS train 0.3745530421845615 valid 0.2742224839893488 ACC train 0.989391493296225 valid 0.9885385603158263\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.2475697219504509\n",
      "31705\n",
      "  batch 2000 loss: 0.24103042201977223\n",
      "63395\n",
      "  batch 3000 loss: 0.2545885468367487\n",
      "95098\n",
      "  batch 4000 loss: 0.35608161372458563\n",
      "126755\n",
      "  batch 5000 loss: 0.26030703240679576\n",
      "158415\n",
      "  batch 6000 loss: 0.244508680286468\n",
      "190125\n",
      "  batch 7000 loss: 0.27332264012207086\n",
      "221762\n",
      "  batch 8000 loss: 0.3927349475338124\n",
      "253410\n",
      "  batch 9000 loss: 0.2675781482057646\n",
      "285063\n",
      "  batch 10000 loss: 0.27008496919507163\n",
      "316688\n",
      "  batch 11000 loss: 0.25445384640851987\n",
      "348314\n",
      "  batch 12000 loss: 0.27345656060054896\n",
      "379967\n",
      "  batch 13000 loss: 0.24570442386216018\n",
      "411655\n",
      "  batch 14000 loss: 0.271655085661856\n",
      "443322\n",
      "  batch 15000 loss: 0.3457089219968766\n",
      "474976\n",
      "  batch 16000 loss: 0.27721081851050255\n",
      "506645\n",
      "  batch 17000 loss: 0.26405901867611103\n",
      "538283\n",
      "  batch 18000 loss: 0.25599669392884244\n",
      "569933\n",
      "  batch 19000 loss: 0.2486451127687469\n",
      "601591\n",
      "  batch 20000 loss: 0.2677597375712357\n",
      "633244\n",
      "  batch 21000 loss: 0.27082261226442644\n",
      "664893\n",
      "  batch 22000 loss: 0.2601280201943591\n",
      "696551\n",
      "  batch 23000 loss: 0.3398946423168436\n",
      "728183\n",
      "LOSS train 0.3398946423168436 valid 0.2805758735673993 ACC train 0.9893211855739167 valid 0.9878752825563255\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.23877299571500044\n",
      "31687\n",
      "  batch 2000 loss: 0.2474112190293381\n",
      "63392\n",
      "  batch 3000 loss: 0.26891011535329745\n",
      "95077\n",
      "  batch 4000 loss: 0.25508817033609377\n",
      "126733\n",
      "  batch 5000 loss: 0.261737284141127\n",
      "158403\n",
      "  batch 6000 loss: 0.2830986411679769\n",
      "190044\n",
      "  batch 7000 loss: 0.2791854704307043\n",
      "221693\n",
      "  batch 8000 loss: 0.30033820923996246\n",
      "253325\n",
      "  batch 9000 loss: 0.25076958769679186\n",
      "284985\n",
      "  batch 10000 loss: 0.25824828670406713\n",
      "316655\n",
      "  batch 11000 loss: 0.27313353192899376\n",
      "348326\n",
      "  batch 12000 loss: 0.3655047202540445\n",
      "379944\n",
      "  batch 13000 loss: 0.26440888721775263\n",
      "411585\n",
      "  batch 14000 loss: 0.24526720509282313\n",
      "443270\n",
      "  batch 15000 loss: 0.2538023684099899\n",
      "474963\n",
      "  batch 16000 loss: 0.26944964914970476\n",
      "506621\n",
      "  batch 17000 loss: 0.2698073143521324\n",
      "538300\n",
      "  batch 18000 loss: 0.2858731456124224\n",
      "569960\n",
      "  batch 19000 loss: 0.27618407028465297\n",
      "601591\n",
      "  batch 20000 loss: 0.2872627378809266\n",
      "633211\n",
      "  batch 21000 loss: 0.2604608514918946\n",
      "664880\n",
      "  batch 22000 loss: 0.2745344483447261\n",
      "696526\n",
      "  batch 23000 loss: 0.27756875035067785\n",
      "728169\n",
      "LOSS train 0.27756875035067785 valid 0.303209649593926 ACC train 0.9893835339314354 valid 0.9886977469781065\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.2483756310619242\n",
      "31697\n",
      "  batch 2000 loss: 0.26036016738414763\n",
      "63376\n",
      "  batch 3000 loss: 0.4562998765809462\n",
      "95019\n",
      "  batch 4000 loss: 0.2506745696647558\n",
      "126682\n",
      "  batch 5000 loss: 0.2532924500450026\n",
      "158382\n",
      "  batch 6000 loss: 0.26128572056474514\n",
      "190061\n",
      "  batch 7000 loss: 0.39641130531474483\n",
      "221701\n",
      "  batch 8000 loss: 0.3705112845049589\n",
      "253342\n",
      "  batch 9000 loss: 0.26840068895946023\n",
      "284984\n",
      "  batch 10000 loss: 0.25107172031118535\n",
      "316632\n",
      "  batch 11000 loss: 0.27149083315717143\n",
      "348267\n",
      "  batch 12000 loss: 0.3068925464351196\n",
      "379898\n",
      "  batch 13000 loss: 0.27248160723247566\n",
      "411527\n",
      "  batch 14000 loss: 0.25066816503019074\n",
      "443188\n",
      "  batch 15000 loss: 0.2565744873047806\n",
      "474802\n",
      "  batch 16000 loss: 0.3702113048993051\n",
      "506402\n",
      "  batch 17000 loss: 0.28627622626681115\n",
      "538058\n",
      "  batch 18000 loss: 0.26882532063737746\n",
      "569704\n",
      "  batch 19000 loss: 0.2667907579448074\n",
      "601330\n",
      "  batch 20000 loss: 0.3481879779896699\n",
      "632976\n",
      "  batch 21000 loss: 0.3011276260567829\n",
      "664588\n",
      "  batch 22000 loss: 0.29227756592491644\n",
      "696237\n",
      "  batch 23000 loss: 0.24813186254241737\n",
      "727881\n",
      "LOSS train 0.24813186254241737 valid 0.29595332627179943 ACC train 0.9889855656919541 valid 0.9884908043171423\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.2599692316721193\n",
      "31663\n",
      "  batch 2000 loss: 0.2539291891454486\n",
      "63333\n",
      "  batch 3000 loss: 0.25123972063139083\n",
      "94982\n",
      "  batch 4000 loss: 0.26249060758119686\n",
      "126638\n",
      "  batch 5000 loss: 0.27112594258924944\n",
      "158315\n",
      "  batch 6000 loss: 0.26034554236661644\n",
      "189978\n",
      "  batch 7000 loss: 0.27586811456363647\n",
      "221625\n",
      "  batch 8000 loss: 0.2657731118847005\n",
      "253299\n",
      "  batch 9000 loss: 0.29435776587133294\n",
      "284899\n",
      "  batch 10000 loss: 0.27437735417697695\n",
      "316518\n",
      "  batch 11000 loss: 0.29115367052285\n",
      "348117\n",
      "  batch 12000 loss: 0.4863122006789199\n",
      "379717\n",
      "  batch 13000 loss: 0.2818154821698554\n",
      "411378\n",
      "  batch 14000 loss: 0.3677824600552849\n",
      "443010\n",
      "  batch 15000 loss: 0.25866639213706366\n",
      "474639\n",
      "  batch 16000 loss: 0.26394921178648795\n",
      "506278\n",
      "  batch 17000 loss: 0.24187113477761157\n",
      "537960\n",
      "  batch 18000 loss: 0.26456852783268914\n",
      "569594\n",
      "  batch 19000 loss: 0.29223505206691336\n",
      "601199\n",
      "  batch 20000 loss: 0.26898587483260783\n",
      "632840\n",
      "  batch 21000 loss: 0.26457291382877157\n",
      "664499\n",
      "  batch 22000 loss: 0.2997683141375759\n",
      "696126\n",
      "  batch 23000 loss: 0.40852984341094273\n",
      "727728\n",
      "LOSS train 0.40852984341094273 valid 0.46105312660089 ACC train 0.9887162738499049 valid 0.9833172377930361\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.2994398294901475\n",
      "31617\n",
      "  batch 2000 loss: 0.2547760175975272\n",
      "63268\n",
      "  batch 3000 loss: 0.2514768781205639\n",
      "94913\n",
      "  batch 4000 loss: 0.2519714861740358\n",
      "126586\n",
      "  batch 5000 loss: 0.2780126910782128\n",
      "158258\n",
      "  batch 6000 loss: 0.25214017024202623\n",
      "189940\n",
      "  batch 7000 loss: 0.27061369699565696\n",
      "221584\n",
      "  batch 8000 loss: 0.2590650109165581\n",
      "253221\n",
      "  batch 9000 loss: 0.27182408154767473\n",
      "284840\n",
      "  batch 10000 loss: 0.2790574599755928\n",
      "316473\n",
      "  batch 11000 loss: 0.3626054200238577\n",
      "348090\n",
      "  batch 12000 loss: 0.2861104981703684\n",
      "379732\n",
      "  batch 13000 loss: 0.2854963057116256\n",
      "411370\n",
      "  batch 14000 loss: 0.27627240159269423\n",
      "443044\n",
      "  batch 15000 loss: 0.27157961726584473\n",
      "474671\n",
      "  batch 16000 loss: 0.2716498851407923\n",
      "506312\n",
      "  batch 17000 loss: 0.2723319525151746\n",
      "537927\n",
      "  batch 18000 loss: 0.28398566057824065\n",
      "569532\n",
      "  batch 19000 loss: 0.2933882713821949\n",
      "601159\n",
      "  batch 20000 loss: 0.31395367097400595\n",
      "632795\n",
      "  batch 21000 loss: 0.28524413978180385\n",
      "664417\n",
      "  batch 22000 loss: 0.2851904452283416\n",
      "696048\n",
      "  batch 23000 loss: 0.2675796381614637\n",
      "727660\n",
      "LOSS train 0.2675796381614637 valid 0.2964912479178974 ACC train 0.9886592317355792 valid 0.9868564879177323\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.2590131730804569\n",
      "31636\n",
      "  batch 2000 loss: 0.29297574487514794\n",
      "63265\n",
      "  batch 3000 loss: 0.3158620978143299\n",
      "94906\n",
      "  batch 4000 loss: 0.5654187426683494\n",
      "126493\n",
      "  batch 5000 loss: 0.42928311842411493\n",
      "158092\n",
      "  batch 6000 loss: 0.24934901756979524\n",
      "189774\n",
      "  batch 7000 loss: 0.2522871051516267\n",
      "221449\n",
      "  batch 8000 loss: 0.3368140812367201\n",
      "253069\n",
      "  batch 9000 loss: 0.26223742676526307\n",
      "284699\n",
      "  batch 10000 loss: 0.2548418886284926\n",
      "316389\n",
      "  batch 11000 loss: 0.26497453585010955\n",
      "348016\n",
      "  batch 12000 loss: 0.26346939380950063\n",
      "379668\n",
      "  batch 13000 loss: 0.29883951022161637\n",
      "411281\n",
      "  batch 14000 loss: 0.30991437370241326\n",
      "442872\n",
      "  batch 15000 loss: 0.2803815287050338\n",
      "474488\n",
      "  batch 16000 loss: 0.265373840129585\n",
      "506117\n",
      "  batch 17000 loss: 0.2586011741287075\n",
      "537756\n",
      "  batch 18000 loss: 0.2858541410774924\n",
      "569331\n",
      "  batch 19000 loss: 0.29621277351898606\n",
      "600922\n",
      "  batch 20000 loss: 0.32131615112163125\n",
      "632552\n",
      "  batch 21000 loss: 0.4597383590360878\n",
      "664151\n",
      "  batch 22000 loss: 0.30743064355906247\n",
      "695766\n",
      "  batch 23000 loss: 0.2735826609279029\n",
      "727411\n",
      "LOSS train 0.2735826609279029 valid 0.30004944639571574 ACC train 0.9883169790496253 valid 0.9875409905655371\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.27716147212847136\n",
      "31644\n",
      "  batch 2000 loss: 0.25665009565342917\n",
      "63303\n",
      "  batch 3000 loss: 0.26050641780355366\n",
      "94953\n",
      "  batch 4000 loss: 0.27057394380639016\n",
      "126611\n",
      "  batch 5000 loss: 0.2746560973860032\n",
      "158238\n",
      "  batch 6000 loss: 0.291689957883209\n",
      "189877\n",
      "  batch 7000 loss: 0.2702270405739546\n",
      "221510\n",
      "  batch 8000 loss: 0.2789672113627894\n",
      "253088\n",
      "  batch 9000 loss: 0.2847202297830954\n",
      "284689\n",
      "  batch 10000 loss: 0.2661498852376535\n",
      "316323\n",
      "  batch 11000 loss: 0.27575102154898923\n",
      "347915\n",
      "  batch 12000 loss: 0.2797069754960248\n",
      "379541\n",
      "  batch 13000 loss: 0.27464470798615365\n",
      "411159\n",
      "  batch 14000 loss: 0.3764967170206364\n",
      "442694\n",
      "  batch 15000 loss: 0.2907910673609003\n",
      "474299\n",
      "  batch 16000 loss: 0.26560367399360985\n",
      "505951\n",
      "  batch 17000 loss: 0.26053264274529647\n",
      "537568\n",
      "  batch 18000 loss: 0.2743317341427319\n",
      "569196\n",
      "  batch 19000 loss: 0.2795617196019739\n",
      "600766\n",
      "  batch 20000 loss: 0.29747065234114417\n",
      "632390\n",
      "  batch 21000 loss: 0.29847113002138215\n",
      "664050\n",
      "  batch 22000 loss: 0.2878561622335401\n",
      "695660\n",
      "  batch 23000 loss: 0.2823937105391597\n",
      "727238\n",
      "LOSS train 0.2823937105391597 valid 0.4939328993981619 ACC train 0.9880569731331642 valid 0.9821764000466947\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.2909344192948192\n",
      "31602\n",
      "  batch 2000 loss: 0.2994753513149917\n",
      "63221\n",
      "  batch 3000 loss: 0.3777576002907335\n",
      "94805\n",
      "  batch 4000 loss: 0.2930303980699869\n",
      "126431\n",
      "  batch 5000 loss: 0.2726746122264303\n",
      "158035\n",
      "  batch 6000 loss: 0.38006001107447446\n",
      "189646\n",
      "  batch 7000 loss: 0.26095168657024626\n",
      "221262\n",
      "  batch 8000 loss: 0.2727561815662775\n",
      "252875\n",
      "  batch 9000 loss: 0.2648093716863077\n",
      "284506\n",
      "  batch 10000 loss: 0.26201658624911217\n",
      "316122\n",
      "  batch 11000 loss: 0.26605819174842327\n",
      "347763\n",
      "  batch 12000 loss: 0.292513591449233\n",
      "379371\n",
      "  batch 13000 loss: 0.3214299187366851\n",
      "410992\n",
      "  batch 14000 loss: 0.36977201628684997\n",
      "442589\n",
      "  batch 15000 loss: 0.33179160167631927\n",
      "474211\n",
      "  batch 16000 loss: 0.2851313381891814\n",
      "505838\n",
      "  batch 17000 loss: 0.2913693709978834\n",
      "537464\n",
      "  batch 18000 loss: 0.3048054449295742\n",
      "569036\n",
      "  batch 19000 loss: 0.2776898645131805\n",
      "600665\n",
      "  batch 20000 loss: 0.273220605449751\n",
      "632280\n",
      "  batch 21000 loss: 0.286495611436083\n",
      "663878\n",
      "  batch 22000 loss: 0.2871311558764428\n",
      "695458\n",
      "  batch 23000 loss: 0.28753493873681873\n",
      "727080\n",
      "LOSS train 0.28753493873681873 valid 0.35711531109394334 ACC train 0.9878433968446425 valid 0.9860127986076473\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.28772571752103976\n",
      "31608\n",
      "  batch 2000 loss: 0.32608349369210193\n",
      "63210\n",
      "  batch 3000 loss: 0.431781275964473\n",
      "94814\n",
      "  batch 4000 loss: 0.4704231056877179\n",
      "126378\n",
      "  batch 5000 loss: 0.43822475965171637\n",
      "157981\n",
      "  batch 6000 loss: 0.26384516001399605\n",
      "189613\n",
      "  batch 7000 loss: 0.25493850408517027\n",
      "221259\n",
      "  batch 8000 loss: 0.26584504465560893\n",
      "252865\n",
      "  batch 9000 loss: 0.26583121248443786\n",
      "284505\n",
      "  batch 10000 loss: 0.2636021978250428\n",
      "316156\n",
      "  batch 11000 loss: 0.2864513183113886\n",
      "347738\n",
      "  batch 12000 loss: 0.29348349795117973\n",
      "379314\n",
      "  batch 13000 loss: 0.3044686965523288\n",
      "410879\n",
      "  batch 14000 loss: 0.2778640331240422\n",
      "442456\n",
      "  batch 15000 loss: 0.2753198117740976\n",
      "474079\n",
      "  batch 16000 loss: 0.2586579381247866\n",
      "505708\n",
      "  batch 17000 loss: 0.28576142713427544\n",
      "537308\n",
      "  batch 18000 loss: 0.29630061466279584\n",
      "568835\n",
      "  batch 19000 loss: 0.3015481166271493\n",
      "600413\n",
      "  batch 20000 loss: 0.3213890425760765\n",
      "631949\n",
      "  batch 21000 loss: 0.40273607821855695\n",
      "663466\n",
      "  batch 22000 loss: 0.29114270490035415\n",
      "695033\n",
      "  batch 23000 loss: 0.2710310382653552\n",
      "726645\n",
      "LOSS train 0.2710310382653552 valid 0.3358233693625927 ACC train 0.9873379771805012 valid 0.9864266839295758\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "best_vloss = 1_000_000.\n",
    "best_vacc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, accuracy = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    val_accuracy = 0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(testing_loader):\n",
    "            vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)  # Move data to GPU\n",
    "            voutputs = model(vinputs)\n",
    "            \n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            \n",
    "            running_vloss += vloss.item()\n",
    "            # val_accuracy += (torch.topk(voutputs, 1) == torch.argmax(vlabels, 1)).sum().item()\n",
    "            top5_indices = voutputs.topk(5, dim=1).indices\n",
    "            for j in range(len(top5_indices)):\n",
    "                val_accuracy += (top5_indices[j] == torch.argmax(vlabels[j])).sum().item()\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_accuracy /= len(testing_loader.dataset)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss), 'ACC train {} valid {}'.format(accuracy, val_accuracy))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                       epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                       {'Training': accuracy, 'Validation': val_accuracy},\n",
    "                       epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss and val_accuracy > best_vacc:\n",
    "        best_vloss = avg_vloss\n",
    "        best_vacc = val_accuracy\n",
    "        model_path = '../model/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = ANN()\n",
    "PATH = '../model/model_latest.pth'\n",
    "model_load.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (fc1): Linear(in_features=377, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc5): Linear(in_features=1024, out_features=773, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(56)\n"
     ]
    }
   ],
   "source": [
    "NUM = 20000\n",
    "y_pred = model_load(X_test_tensor[:NUM].squeeze(0))\n",
    "_, predicted = torch.max(y_pred, 1)\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_tests = torch.max(y_test_ohe_tensor[:NUM], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9023\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_tests == predicted).sum().item() / y_test_ohe_tensor[:NUM].size(0)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([753829, 377])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
